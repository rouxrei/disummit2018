{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiSummit 2018 - Introduction to text mining \n",
    "# Exercises\n",
    "<img src=\"XploData_logo.png\" width = \"40%\">\n",
    "<br>\n",
    "Powered by [XploData](https://www.xplodata.be/)\n",
    "<br>\n",
    "Reinert Roux - Jellert Schaepherders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text mining is a general term used to indicate the retrieval of information from (large quantities of) text. What can be done by hand (reading text, knowing what it is about, determining keywords, associating it with other texts...) can to some extent also be done by using machine learning algorithms, be it much faster. In this hands-on Python workshop, we will introduce you to some of the basic text mining principles.\n",
    "\n",
    "In the context of the [HIV Hackathon](https://hivhack.org/), organized by DigitYser in September 2018, we will mine text from multiple PDFs related to HIV. For example, this [publicly available PDF](http://phia.icap.columbia.edu/wp-content/uploads/2017/11/Tanzania_SummarySheet_A4.English.v19.pdf) related to HIV in Tanzania. Although the PDFs contains multiple types of data, the scope of this workshop will be limited to the actual text itself. \n",
    "<br>\n",
    "*Note that text can also be extracted from images using [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition).*\n",
    "\n",
    "Before we can start with text mining, we need to acquire the text from our PDF and load it into Python.\n",
    "Fortunately, multiple Python libraries exist. We will use the popular PyPDF2 package.\n",
    "For flexibility reasons, we will also introduce the XpdfReader toolkit.\n",
    "\n",
    "### Exercises\n",
    "This notebook is the exercise version of the workshop. In this version some code parts are left out. You need to fill those out yourself. Feel free to check the solution version (afterwards).\n",
    "\n",
    "Places where your input is expected will look like this:\n",
    "\n",
    "`var = ##your code##`\n",
    "\n",
    "Make sure to replace all the `#` so that the code can be executed.\n",
    "\n",
    "\n",
    "## Importing data\n",
    "\n",
    "In these exercises we assume that we already have a txt version of all our pdf files.\n",
    "For more information on how to achieve those txt files, look at the solution version of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Work\\01.Internal Projects\\03.Data Science\n"
     ]
    }
   ],
   "source": [
    "# Project setup\n",
    "import os\n",
    "\n",
    "# change working directory\n",
    "os.chdir('..')\n",
    "project_root = os.getcwd()\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Work\\\\01.Internal Projects\\\\03.Data Science\\\\output\\\\output_txt\\\\guess_encoding.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c89efb189b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# read file as bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mraw_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbyte_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Work\\\\01.Internal Projects\\\\03.Data Science\\\\output\\\\output_txt\\\\guess_encoding.txt'"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# define input file path\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# read file as bytes\n",
    "with open(txt_file, mode='rb') as byte_file:\n",
    "    raw_input = byte_file.read()\n",
    "\n",
    "# guess encoding\n",
    "encodings = chardet.detect(raw_input)\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tant national and regional HIV-related parameters, including progress toward 90-90-90 goals, and will guide policy and funding priorities. ICAP at Columbia University is implementing the PHIA Project in close collaboration with CDC and other partners. See phia.icap.columbia.edu for more details.\n",
      "The mark \"CDC\" is owned by the US Dept. of Health and Human Services and is used with permission. Use of this logo is not an endorsement by HHS or CDC of any particular product, service, or enterprise. This project is supported by the U.S. President's Emergency Plan for AIDS Relief (PEPFAR) through CDC under the terms of cooperative agreement #U2GGH001226. The findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the funding agencies. The results presented should be considered preliminary and they are subject to change.\n",
      "6\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# decode bytes to string\n",
    "text = raw_input.decode('8859')\n",
    "# show text sample\n",
    "print(text[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the examples, we will continue using the `ISO-8859-1` decoded text.\n",
    "\n",
    "For more information on supported codecs, visit the python [docs](https://docs.python.org/3/library/codecs.html#standard-encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection\n",
    "A human capable of reading is able to distinguish between his mother tongue and a foreign language. We perceive this by reading language specific words, grammatical constructions, context... Language detection in Python is quite straightforward and is performed using the `langdetect` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-93d5fe648379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "language = detect(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Until this point, you were mainly preparing the data, i.e. converting the pdf to text and analyzing some meta-data.\n",
    "The next critical step in text mining is preprocessing. This process involves different techniques (for an overview, see [here](https://pdfs.semanticscholar.org/1fa1/1c4de09b86a05062127c68a7662e3ba53251.pdf)), of which we will cover **tokenization, part-of-speech tagging, stop word removal, stemming and lemmatization**.\n",
    "### Wordclouds and bar charts\n",
    "To get insights in the contents of a text file, various visualisations are possible. Frequently used visualisations are wordclouds and bar charts. These will therefore be used to demonstrate the differenct aspects of text mining preprocessing.\n",
    "\n",
    "*Since we will often repeat the same visualisation during this workshop, we prepared a custom function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# instantiate wordcloud\n",
    "wordcloud = WordCloud(  background_color='white',\n",
    "                        max_words=50,\n",
    "                        max_font_size=80, \n",
    "                        random_state=42,\n",
    "                        collocations=False)\n",
    "\n",
    "def gen_wc_barh(tokens, title='Default title', amount=9):\n",
    "    \"\"\"\n",
    "    Generate a horizontal bar graph from the top #amount tokens.\n",
    "    Generate a wordcloud from the provided tokens.\n",
    "    Show both vizualizations.\n",
    "    \"\"\"\n",
    "    # count tokens\n",
    "    ctr = Counter(tokens)\n",
    "    \n",
    "    # initialize plt figure\n",
    "    plt.figure(figsize=(12,9))\n",
    "    \n",
    "    # generate barh from counted tokens\n",
    "    tokens, weights = zip(*ctr.most_common(amount))\n",
    "    plt.subplot2grid((3, 3), (0, 0))\n",
    "    plt.barh(tokens, weights)\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Token count bar-graph')\n",
    "    \n",
    "    # generate wc from the counted tokens\n",
    "    wordcloud.generate_from_frequencies(dict(ctr))\n",
    "    plt.subplot2grid((3, 3), (0, 1), colspan=2)\n",
    "    plt.imshow(wordcloud, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word cloud')\n",
    "\n",
    "    # general title\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "        \n",
    "    # show both graphs\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the importance of preprocessing, we will generate a visualisation after each preprocessing step, whereafter you can evaluate the effect. For starters, we build our first visual on the *unpreprocessed* text, to get a feeling of what a wordcloud can tell you about the contents of a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from unprocessed input text\n",
    "wordcloud.generate_from_text(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how frequent occurring words are displayed in a larger font size. Yet these are not necessarily the most important words (e.g. percent, year, among). Luckily, you can improve this! Let's take it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "[Tokenization]( https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of cutting the text files in individual *tokens*. These can be words, but also numbers, punctuation marks or symbols. However, don’t you worry about that just yet. To tokenize, you’ll need an additional package, called the \"Natural Language Toolkit\": `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Use `nltk.sent_tokenize` to split the text into sentences.\n",
    "\n",
    "Use `nltk.word_tokenize` to split a sentence into tokens.\n",
    "\n",
    "*Note that both functions take a `str` as input and return a list, so you will need to use a list-comprehension to apply `nltk.word_tokenize` on an individual sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# get the input text\n",
    "text = raw_input.decode('8859')\n",
    "\n",
    "# break text up into smaller bits -> tokens\n",
    "# First, split the text in sentences\n",
    "sentences = ##your code##\n",
    "# Then we further split each sentence into tokens\n",
    "tokenized_sentences = [##your code##]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate all the tokenized sentences into a single list\n",
    "gen_wc_barh([token for sent in tokenized_sentences for token in sent], title='Tokenized text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, you can see that the extracted tokens resulted in more than just 'words'. What do you think was used as delimiter (or separator) for splitting the tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tagging\n",
    "To increase the value of our freshly extracted tokens, we can assign labels/tags to them. As a result, each token is given a grammatical meaning. This process is called [Part-Of-Speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). The ‘tagging’ is based on a previously trained model, which we can also call from the `nltk` package. The default tags generated by `nltk` can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "#### Exercise\n",
    "Use `nltk.pos_tag` to perform POS tagging on the tokens.\n",
    "\n",
    "This functions expects a list of tokens and returns a list of tuples `(token, tag)`.\n",
    "\n",
    "*So again, you will have to use a list-comprehension to apply `nltk.pos_tag` on a sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model built into NLTK to assign grammatical meaning to our tokens -> POS tags\n",
    "tagged_sentences = [##your code##]\n",
    "print(\"Example tagged sentence:\")\n",
    "print(tagged_sentences[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing our text, we usually aren't interested in all types of tokens. For our usecase we are interested in discovering the topics of our text, so we are more interested in 'word-like' tokens. Therefore, let's select the appropriate tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which tokens we want to consider\n",
    "# shorthand POS tag lists:\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# interesting tokens\n",
    "tokens = [(token, tag)\\\n",
    "            for sentence in tagged_sentences\\\n",
    "            for (token, tag) in sentence\\\n",
    "            if token.isalpha()\\\n",
    "            and tag in adj + noun + adverb + verb]\n",
    "\n",
    "# extract the 'token-values' from the tagged sentences\n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Filtered tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tagging is not always ‘sure’ about the meaning of a word. Could you determine which (of the) word(s) in the above bar chart has/have multiple tagging possibilities?\n",
    "\n",
    "#### Exercise\n",
    "Different textmining applications require different preprocessing.\n",
    "For example in sentiment analysis we're mostly interested in adjectives.\n",
    "Or maybe we're analyzing certain verb usage per category etc.\n",
    "\n",
    "**Apply different filters on the code above to get a different set of tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence. These words are called stop words. How many stop words do you think are in this paragraph? To better understand the content of a text, you want to filter out those stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# filter out the english stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokens = [(token, tag) for (token,tag) in tokens if token not in stopwords]\n",
    "\n",
    "# extract the 'token-values' from the filtered tokens \n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Stopwords removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the changes in the wordcloud with and without the removal of stop words? Which words were omitted?\n",
    "\n",
    "#### Exercise\n",
    "Check if you guessed correctly which words in the paragraph above are stopwords by filtering them out using the `stopwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence. These words are called stop words. How many stop words do you think are in this paragraph? To better understand the content of a text, you want to filter out those stop words.\"\n",
    "\n",
    "# tokenize the paragraph\n",
    "paragraph_tokenized = ##your code##\n",
    "\n",
    "# remove the stopwords\n",
    "paragraph_no_stops = ##your code##\n",
    "\n",
    "# print the result\n",
    "print(' '.join([token for (token, tag) in paragraph_no_stops]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is a preprocessing step whereby inflected words are reduced to their word stem, base or root form. To give an example, the words “computational”, “computers” and “computation” would, according to a predefined algorithm, result in the stem word “comput” after the stemming process. Consequently, words with a similar stem will be grouped together and won’t skew the word count/frequency distribution. However, “comput” isn’t a real word and could obscure the interpretation of the text. Nevertheless, the code is not very complicated as shown below:\n",
    "\n",
    "#### Exercise\n",
    "Use the method `stem` of the initialized stemmer to stem all the `tokens`.\n",
    "\n",
    "It takes a `str` as an input and also returns a `str`.\n",
    "\n",
    "*Note that `tokens` is a list of tuples `(token, tag)`, so again list-comprehensions are our friend and make sure you apply the method to the token only, not the whole tuple.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stem tokens using the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [##your code##]\n",
    "\n",
    "gen_wc_barh(stemmed_tokens, title='Stemmed tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a side note: stemming could be followed by a stem completion algorithm, which completes the stemmed words (e.g. \"comput\") to their meaningful counterparts (e.g. \"computer\"), based on a predefined completion dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "While stemming uses predetermined rules to get to the *stem* version of a word, [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is based on a trained model and is aware of word *meanings*. For example: “are” and “be” will remain two different words after stemming, but will be changed into 'is' after the lemmatization process. Still, a well-trained algorithm can distinguish between similar words with different meanings (e.g. 'viral' and 'virally'), so they are not combined during lemmatization.\n",
    "For our lemmatization we are using the 'wordnet' model. When using this for the first time, you need to **download this model first**.\n",
    "\n",
    "#### Exercise\n",
    "Use the method `lemmatize` of the initialized lemmatizer to lemmatize all the `tokens`.\n",
    "\n",
    "It takes as input a `str` and returns a `str`.\n",
    "\n",
    "*Note that `tokens` is a list of tuples `(token, tag)`, so again list-comprehensions are our friend and make sure you apply the method to the token only, not the whole tuple.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem.wordnet as wordnet\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize interesting tokens\n",
    "lemmatizer = wordnet.WordNetLemmatizer()\n",
    "lemmatized_tokens = [##your code##]\n",
    "gen_wc_barh(lemmatized_tokens, title='Lemmatized tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main differences between these visuals and the previous ones? Which one is more useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "So far, what you mainly have been doing, is a so called [bag-of-words analysis](https://en.wikipedia.org/wiki/Bag-of-words_model). This simplified model literally throws all words (or tokens, if you wish) of a document into a ‘bag’ and then looks for the most occurring ones. Now, we will do this for a range of documents.\n",
    "\n",
    "#### Document Term Matrix\n",
    "Before being able to do so, one has to construct a [document-term-matrix **(DTM)**](https://en.wikipedia.org/wiki/Document-term_matrix).\n",
    "Simply put, A DTM is just another mathematical representation of multiple bag of word analyses, with rows corresponding to the documents in the corpus and columns corresponding to the terms (tokens/words). The values are the term (word/token) frequencies per document. One can also normalize these frequencies by dividing by the total number of words per document.\n",
    "\n",
    "With `sklearn`, you can use `vectorizers` to create a DTM to go from text or tokens to a ([sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)) matrix representing such a DTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hier graag een oefening: zelf voordoen hoe de DTM af te beelden (puur als matrix), en ze de opdracht geven om deze te normaliseren (dus gedeeld door het totaal aantal woorden per document)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Set, Counter\n",
    "\n",
    "# get a set of unique tokens in our documents\n",
    "set_of_words = set([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The documents contain {} unique words'.format(len(set_of_words)))\n",
    "\n",
    "# count the tokens using a 'Counter' collection\n",
    "count_of_words = Counter([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The most common words are:')\n",
    "print(count_of_words.most_common(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF) vs Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "Instead of considering the raw term frequencies per document, one could also account for the frequencies in the corpus. The sparseness of a term is measured by its inverse document frequency. The product of the TF and IDF gives you the TF-IDF. Whichever you use, depends on the purpose. Rare terms are useful for information retrieval, yet not for classification or clustering. Common terms as well are not useful for classification or clustering. For information retrieval, one should thus use the TF.\n",
    "\n",
    "Let us compare a sample of the term-frequency matrix with the same sample from the term-frequency-inverse-document-frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hier graag een oefening: ze zelf de TFIDF laten afbeelden (puur als matrix)**\n",
    "**Dan als tweede oefening de vergelijking tussen de TF en de TFIDF (zoals we vrijdag in die bar charts gezien hadden, misschien zelf de TF maken en ze dan zelf de TFIDF laten maken en verschillen analyseren **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Term frequency')\n",
    "document_amount = tf_segm.shape[0]\n",
    "token_amount = tf_segm.shape[1]\n",
    "print((tf_segm/token_amount * document_amount)[:,:6].todense())\n",
    "print()\n",
    "\n",
    "print('Term frequency - Inverse document frequency')\n",
    "print(tfidf[:,:6].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the *regular* term-frequency matrix because we want to use LDA in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "After all preprocessing steps, the *real* fun begins. Where POS tagging was limited to tag individual words only, [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is able to group multiple tokens into predefined categories, such as person names, locations, organizations, products, time... \n",
    "We'll give an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "# take a 'random' sentence\n",
    "sentence = list(chunked_sentences)[-4]\n",
    "print(sentence)\n",
    "# You can get a more graphical drawing (will show up in a pop-up window)\n",
    "#sentence.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, you can recognize the found 'Named Entities' by the prefix `(NE`. We can now use the same wordcloud and bar chart visualizations to highlight the most frequent occurring named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "# filter out everything else\n",
    "named_entities = [chunk.leaves() for sent in chunked_sentences for chunk in sent if hasattr(chunk, \"label\") and chunk.label() == \"NE\"]\n",
    "NE_tokens = [token for leaves in named_entities for (token, tag) in leaves]\n",
    "\n",
    "gen_wc_barh(NE_tokens, title='Named Entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Can you give all titles of the documents containing the word HIV more than 15 times?\n",
    "\n",
    "**Dit moet nog uitgewerkt worden**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic detection & prediction\n",
    "Instead of doing a simple bag-of-words analysis, let's do something more fancy: topic detection! \n",
    "\n",
    "To this end, we will be using the popular [latent Dirichlet allocation model **(LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "This model can also be used to predict the topics of new, unseen documents.\n",
    "\n",
    "An LDA model is essentially used to discover topics in documents, based on a variety of variables, such as the number of topics, number of documents, probability and distribution of words, identity and weights of the words etc. The math behind the model is mind-boggling (have a look [here]( https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution). Luckily for us, calling the LDA model in Python is not that hard.\n",
    "\n",
    "*Note that for the purpose of this workshop we will keep the model simple. As an exercise you can apply the various preprocessing techniques we discussed earlier  (notably stemming or lemmatizing) and experiment with the parameters of the model itself.*\n",
    "\n",
    "We will use the DTM we computed earlier to train the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ik zou dit nog opsplitsen in**\n",
    "- **een train stap waaruit dan de verschillende topics al eens geprint worden.**\n",
    "- **vermelding maken van het feit dat met deze topics even afgestemt moet worden met de business users**\n",
    "- **een nieuw document erdoor jassen om dan te kijken hoeveel procent bij welke topic hoort.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c0dd8813289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#txt_dir = r'..\\output\\medicine_txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtxt_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medicine_txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project_root' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#txt_dir = r'..\\output\\medicine_txt'\n",
    "txt_dir = os.path.join(project_root, 'output', 'medicine_txt')\n",
    "\n",
    "file_name = []\n",
    "input_text = []\n",
    "\n",
    "# read all txt files (one time)\n",
    "for curr_dir,_,filenames in os.walk(txt_dir):\n",
    "    for filename in filenames:\n",
    "        # filter to select only the pdfs that were converted using the 'simple' option\n",
    "        if filename[:7] == 'simple_':\n",
    "            # decode using the iso-8859 character set\n",
    "            with open(os.path.join(curr_dir, filename), 'rt', encoding='8859') as file:\n",
    "                txt_input = file.read()\n",
    "                # consider only files with at least 100 characters\n",
    "                if len(txt_input) > 99:\n",
    "                    # strip '.txt' from the filename\n",
    "                    file_name.append(filename[7:-4])\n",
    "                    input_text.append(txt_input)\n",
    "\n",
    "# store as pd.DataFrame\n",
    "df = pd.DataFrame({'filename':file_name, 'text':input_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "df['tokens'] = df['text'].apply(lambda txt: [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(txt)])\n",
    "\n",
    "# POS-tagging\n",
    "df['POS_tags'] = df['tokens'].apply(lambda tokens: [nltk.pos_tag(sent) for sent in tokens])\n",
    "\n",
    "# split on tags\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "#df['adj'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adj])\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#df['noun'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in noun])\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "#df['adverb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adverb])\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "#df['verb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in verb])\n",
    "\n",
    "# select only tokens that were tagged as 'adjective', 'noun' or 'verb'.\n",
    "df['i_tokens'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if token.isalpha() and tag in adj + noun + verb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term-matrix from the full text\n",
    "tf_full_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_full = tf_full_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Create a document-term-matrix from the selected tokens only\n",
    "tf_segm_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_segm = tf_segm_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))\n",
    "\n",
    "# Create a Term frequency-inverse document frequency (tf-idf) matrix from the selected tokens\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "#tfidf = tfidf_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init LDA to look for 3 topics\n",
    "lda_full = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# a second model for the segmented DTM\n",
    "lda_segm = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# fit the model and transform for results\n",
    "lda_full_results = lda_full.fit_transform(tf_full)\n",
    "lda_segm_results = lda_segm.fit_transform(tf_segm)\n",
    "\n",
    "# read in a new file\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "with open(txt_file, mode='rt', encoding='8859') as byte_file:\n",
    "    new_text = byte_file.read()\n",
    "\n",
    "# transform the new text to a doc-term-matrix\n",
    "tf_full_new = tf_full_vectorizer.transform([new_text])\n",
    "tf_segm_new = tf_segm_vectorizer.transform([new_text])\n",
    "# and transform to discover associated topics\n",
    "lda_full_new = lda_full.fit_transform(tf_full_new)\n",
    "lda_segm_new = lda_segm.fit_transform(tf_segm_new)\n",
    "print(lda_full_new)\n",
    "print(lda_segm_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(lda_full, tf_full_vectorizer.get_feature_names(), 9)\n",
    "print_top_words(lda_segm, tf_segm_vectorizer.get_feature_names(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier komt nog feedback op de gegeven topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other\n",
    "\n",
    "Usefull libs:\n",
    "\n",
    "- re\n",
    "- gensim\n",
    "- spaCy\n",
    "- polyglot\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "Popular NL classifier:\n",
    "- Naive-Bayes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
