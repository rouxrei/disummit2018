{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiSummit 2018 - Introduction to text mining\n",
    "<img src=\"XploData_logo.png\" width = \"40%\">\n",
    "<br>\n",
    "Powered by [XploData](https://www.xplodata.be/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text mining is a general term to retrieve information from (large quantities of) text. What can be done by hand (reading a text, knowing what it is about, determining keywords, associating with other texts...) can to some extent also be done my using machine learning algorithms, be it much faster. In this hands-on Python workshop, we will introduce you to some of the text mining principles that are used to achieve this.\n",
    "\n",
    "In the context of the [HIV Hackathon](https://hivhack.org/), organized by Digityser in September 2018, we will focus on a [public available PDF](http://phia.icap.columbia.edu/wp-content/uploads/2017/11/Tanzania_SummarySheet_A4.English.v19.pdf) related to HIV in Tanzania. Although the PDF contains multiple types of data, the scope of this workshop will be limited to the actual text itself. \n",
    "*As a side note, alternatively text can be extracted from images using [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition).*\n",
    "\n",
    "Before we can start with text mining, we need to acquire the text from our PDF into Python.\n",
    "Fortunately, multiple Python libraries exist, but we will use the popular PyPDF2 package.\n",
    "For flexibility reasons, we will also introduce the XpdfReader toolkit.\n",
    "\n",
    "## Importing data\n",
    "### Extracting text with a python library\n",
    "e.g.: [PyPDF2](https://pypi.org/project/PyPDF2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project setup\n",
    "import os\n",
    "\n",
    "# change working directory\n",
    "os.chdir('..')\n",
    "project_root = os.getcwd()\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# creating an object \n",
    "file_path = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "file = open(file_path, 'rb')\n",
    "\n",
    "# creating a pdf reader object\n",
    "fileReader = PyPDF2.PdfFileReader(file)\n",
    "\n",
    "# print the number of pages in pdf file\n",
    "print(fileReader.numPages)\n",
    "print()\n",
    "\n",
    "# print text on last page\n",
    "print(fileReader.pages[-1].extractText())\n",
    "\n",
    "# close the file-stream\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "    - no control over character encodings\n",
    "    - no image extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text with an external toolkit\n",
    "e.g.: [PDFTOTEXT (XpdfReader)](http://www.xpdfreader.com/)\n",
    "\n",
    "The commandline can be accessed using the `subprocess` standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'utf8_encoded.txt')\n",
    "\n",
    "# make and execute command to convert to txt using UTF-8 encoding for resulting txt file\n",
    "pdf_txt_command = [\"pdftotext\", \"-enc\", \"UTF-8\", in_file, txt_file]\n",
    "subprocess.run(pdf_txt_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now open the textfile with the specified encoding\n",
    "with open(txt_file, mode='rt', encoding='utf-8') as file:\n",
    "    print(file.read()[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You noticed that we used the argument 'UTF-8' for the encoding parameter. Variable width character encoding is used to represent a repertoire of characters by an encoding system (more on that [here](https://en.wikipedia.org/wiki/Character_encoding)). UTF-8 is one of the most common used encoding since 2009, but if you're not sure of the character encoding in the pdf-file you can choose to not specify it during our conversion and make a *guess* later.\n",
    "We can use the `chardet` package for *guessing* character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# make and execute command to convert to txt, without specifying an encoding\n",
    "pdf_txt_command = [\"pdftotext\", in_file, txt_file]\n",
    "subprocess.run(pdf_txt_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# define input file path\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# read file as bytes\n",
    "with open(txt_file, mode='rb') as byte_file:\n",
    "    raw_input = byte_file.read()\n",
    "\n",
    "# guess encoding\n",
    "encodings = chardet.detect(raw_input)\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode bytes to string\n",
    "text = raw_input.decode('8859')\n",
    "# show text sample\n",
    "print(text[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue the rest of the examples with the `ISO-8859-1` decoded text.\n",
    "\n",
    "For more info on supported codecs, visit the python [docs](https://docs.python.org/3/library/codecs.html#standard-encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting images from a PDF\n",
    "#### Using  [PDFIMAGES](http://www.xpdfreader.com/)\n",
    "\n",
    "PDFimages comes from the same toolkit as pdftotext and we also call it using the `subprocess` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "images = os.path.join(project_root, 'output', 'output_img')\n",
    "\n",
    "# extract images\n",
    "pdf_img_command = [\"pdfimages\", \"-j\", in_file, os.path.join(images + 'img')]\n",
    "subprocess.run(pdf_img_command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that a lot of the extracted images are in a ppm or similar format, which few programs can easily work with.\n",
    "A usefull toolkit for working with images is [imagemagick](https://www.imagemagick.org/script/index.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection\n",
    "A human capable of reading is able to distinguish between his mother tongue and a foreign language. We perceive this by reading language specific words, grammatical constructions, context... Language detection in Python is quite straightforward and is performed using the `langdetect` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "language = detect(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Until this point, you were mainly preparing the data, i.e. converting the pdf to text and analyzing some meta-data.\n",
    "The next critical step in text mining is preprocessing. This process involves different techniques (for an overview, see [here](https://pdfs.semanticscholar.org/1fa1/1c4de09b86a05062127c68a7662e3ba53251.pdf)), of which we will cover **tokenization, part-of-speech tagging, stop word removal, stemming and lemmatization**.\n",
    "### Wordclouds and bar charts\n",
    "To get insights of the contents of a text file, various visualisations are possible. Frequently used visualisations are wordclouds and bar charts, and will be used to demonstrate the differenct aspects of text mining preprocessing.\n",
    "\n",
    "*Since we will often repeat the same visualisation during this workshop, we prepared a custom function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# instantiate wordcloud\n",
    "wordcloud = WordCloud(  background_color='white',\n",
    "                        max_words=50,\n",
    "                        max_font_size=80, \n",
    "                        random_state=42,\n",
    "                        collocations=False)\n",
    "\n",
    "def gen_wc_barh(tokens, title='Default title', amount=9):\n",
    "    \"\"\"\n",
    "    Generate a horizontal bar graph from the top #amount tokens.\n",
    "    Generate a wordcloud from the provided tokens.\n",
    "    Show both vizualizations.\n",
    "    \"\"\"\n",
    "    # count tokens\n",
    "    ctr = Counter(tokens)\n",
    "    \n",
    "    # initialize plt figure\n",
    "    plt.figure(figsize=(12,9))\n",
    "    \n",
    "    # generate barh from counted tokens\n",
    "    tokens, weights = zip(*ctr.most_common(amount))\n",
    "    plt.subplot2grid((3, 3), (0, 0))\n",
    "    plt.barh(tokens, weights)\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Token count bar-graph')\n",
    "    \n",
    "    # generate wc from the counted tokens\n",
    "    wordcloud.generate_from_frequencies(dict(ctr))\n",
    "    plt.subplot2grid((3, 3), (0, 1), colspan=2)\n",
    "    plt.imshow(wordcloud, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word cloud')\n",
    "\n",
    "    # general title\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "        \n",
    "    # show both graphs\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the importance of preprocessing, we will generate a visualisation after each preprocessing step, whereafter you can evaluate the effect. For starters, we build our first visual on the *unpreprocessed* text, to get a feeling of what a wordcloud can tell you about the contents of a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from unprocessed input text\n",
    "wordcloud.generate_from_text(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how frequent occurring words are displayed in a larger font size, but these are not necessarily the most important ones (e.g. percent, year, among). You can improve this, let’s do this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "[Tokenization]( https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of cutting the text files in individual ‘tokens’. These can be words, but also numbers, punctuation marks or symbols, but don’t worry for that now. You’ll need an additional package for this, called the \"Natural Language Toolkit\": `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# break text up into smaller bits -> tokens\n",
    "# First, split the text up in sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "# Then we further split each sentence into tokens\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate all the tokenized sentences into a single list\n",
    "gen_wc_barh([token for sent in tokenized_sentences for token in sent], title='Tokenized text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, you can see that the extracted tokens resulted in more than just 'words'. What do you think was used as delimiter (or separator) for splitting up the tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tagging\n",
    "To increase the value of our freshly extracted tokens, we can assign labels/tags to them. As a result, each token is given a grammatical meaning. This process is called [Part-Of-Speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). The ‘tagging’ is based on a previously trained model, which we can also call from the `nltk` package. The default tags generated by `nltk` can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model built into NLTK to assign grammatical meaning to our tokens -> POS tags\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "print(\"Example tagged sentence:\")\n",
    "print(tagged_sentences[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing our text, we usually aren't interested in all types of tokens. For our usecase we are interested in discovering the topics of our text, so we are more interested in 'word-like' tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which tokens we want to consider\n",
    "# shorthand POS tag lists:\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# interesting tokens\n",
    "tokens = [(token, tag)\\\n",
    "            for sentence in tagged_sentences\\\n",
    "            for (token, tag) in sentence\\\n",
    "            if token.isalpha()\\\n",
    "            and tag in adj + noun + adverb + verb]\n",
    "\n",
    "# extract the 'token-values' from the tagged sentences\n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Filtered tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tagging is not always ‘sure’ about the meaning of a word. Could you determine which (of the) word(s) in the above bar chart has/have multiple tagging possibilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence (how many stop words are in this sentence?). To better understand the content of a text, you want to filter out those stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# filter out the english stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokens = [(token, tag) for (token,tag) in tokens if token not in stopwords]\n",
    "\n",
    "# extract the 'token-values' from the filtered tokens \n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Stopwords removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the changes in the wordcloud with and without the removal of stop words? Which words were omitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is a preprocessing step whereby inflected words are reduced to their word stem, base or root form. To give an example, the words “computational”, “computers” and “computation” would, according to a predefined algorithm, result in the stem word “comput” after the stemming process. Consequently, words with a similar stem will be grouped together and won’t skew the word count/frequency distribution. However, “comput” isn’t a real word and could obscure the interpretation of the text. Nevertheless, the code is uncomplicated as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stem tokens using the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for (token, tag) in tokens]\n",
    "\n",
    "gen_wc_barh(stemmed_tokens, title='Stemmed tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a side note: stemming could be followed by a stem completion algorithm, which completes the stemmed words (e.g. \"comput\") to their meaningful counterparts (e.g. \"computer\"), based on a predefined completion dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "While stemming uses predetermined rules to get to the *stem* version of a word, [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is based on a trained model and is aware of word *meanings*. For example: “are” and “be” will remain two different words after stemming, but will be changed into 'is' after the lemmatization proces. Still, a well-trained algorithm can distinguish between similar words with different meanings (e.g. 'viral' and 'virally'), so they are not combined during lemmatization.\n",
    "For our lemmatization we are using the 'wordnet' model. When using this for the first time, you need to **download this model first**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem.wordnet as wordnet\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize interesting tokens\n",
    "lemmatizer = wordnet.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for (token, tag) in tokens]\n",
    "gen_wc_barh(lemmatized_tokens, title='Lemmatized tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main differences between these visuals and the previous ones? Which one is more useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "After al preprocessing steps, the fun *really* begins. Where POS tagging was limited to tag individual words only, [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is able to group multiple tokens into predefined categories, such as person names, locations, organizations, products, time... \n",
    "We'll give an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "# take a 'random' sentence\n",
    "sentence = list(chunked_sentences)[-4]\n",
    "print(sentence)\n",
    "# You can get a more graphical drawing (will show up in a pop-up window)\n",
    "#sentence.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, you can recognize the found 'Named Entities' by the prefix `(NE`. We can now use the same wordcloud and bar chart visualizations to highlight the most frequent occurring named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "# filter out everything else\n",
    "named_entities = [chunk.leaves() for sent in chunked_sentences for chunk in sent if hasattr(chunk, \"label\") and chunk.label() == \"NE\"]\n",
    "NE_tokens = [token for leaves in named_entities for (token, tag) in leaves]\n",
    "\n",
    "gen_wc_barh(NE_tokens, title='Named Entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classification example\n",
    "### Using topic detection\n",
    "So far, what you mainly have been doing, is a so called [bag-of-words analysis](https://en.wikipedia.org/wiki/Bag-of-words_model). This simplified model literally throws all words (or tokens, if you wish) of a document into a ‘bag’ and then looks for the most occurring ones. Now, we will do this for a range of multiple documents, categorizing topics for each document. As a result, you should be able to predict the topics of new, unseen documents.\n",
    "For this, you will need to know about two more text mining techniques: *(i)* convert the bag of words to a [document-term-matrix** (DTM)**](https://en.wikipedia.org/wiki/Document-term_matrix) and *(ii)* the [latent Dirichlet allocation model **(LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "\n",
    "Simply put, A DTM is just another mathematical representation of multiple bag of word analyses, with rows corresponding to the documents in the collection and columns corresponding to the tokens. In Python, you can use the `CountVectorizer` package to create a DTM.\n",
    "An LDA model is essentially used to discover topics in documents, based on a variety of variables, such as the number of topics, number of documents, probability and distribution of words, identity and weights of the words… The math behind the model is mind-boggling (have a look [here]( https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution). Luckily for us, calling the LDA model in Python is not that hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#txt_dir = r'..\\output\\medicine_txt'\n",
    "txt_dir = os.path.join(project_root, 'output', 'medicine_txt')\n",
    "\n",
    "file_name = []\n",
    "input_text = []\n",
    "\n",
    "# read all txt files (one time)\n",
    "for curr_dir,_,filenames in os.walk(txt_dir):\n",
    "    for filename in filenames:\n",
    "        # filter to select only the pdfs that were converted using the 'simple' option\n",
    "        if filename[:7] == 'simple_':\n",
    "            # decode using the iso-8859 character set\n",
    "            with open(os.path.join(curr_dir, filename), 'rt', encoding='8859') as file:\n",
    "                txt_input = file.read()\n",
    "                # consider only files with at least 100 characters\n",
    "                if len(txt_input) > 99:\n",
    "                    # strip '.txt' from the filename\n",
    "                    file_name.append(filename[7:-4])\n",
    "                    input_text.append(txt_input)\n",
    "\n",
    "# store as pd.DataFrame\n",
    "df = pd.DataFrame({'filename':file_name, 'text':input_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "df['tokens'] = df['text'].apply(lambda txt: [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(txt)])\n",
    "\n",
    "# POS-tagging\n",
    "df['POS_tags'] = df['tokens'].apply(lambda tokens: [nltk.pos_tag(sent) for sent in tokens])\n",
    "\n",
    "# split on tags\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "#df['adj'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adj])\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#df['noun'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in noun])\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "#df['adverb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adverb])\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "#df['verb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in verb])\n",
    "\n",
    "# select only tokens that were tagged as 'adjective', 'noun' or 'verb'.\n",
    "df['i_tokens'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if token.isalpha() and tag in adj + noun + verb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "During visualisation in the previous steps we already implicitly used bag-of-words to pass onto the wordcloud and bar graph generators. Basically it boils down to counting the tokens we are interested in.\n",
    "Python has some useful built-in classes for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Set, Counter\n",
    "\n",
    "# get a set of unique tokens in our documents\n",
    "set_of_words = set([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The documents contain {} unique words'.format(len(set_of_words)))\n",
    "\n",
    "# count the tokens using a 'Counter' collection\n",
    "count_of_words = Counter([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The most common words are:')\n",
    "print(count_of_words.most_common(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency matrix\n",
    "We can combine the bags-of-words into DTMs, however `sklearn` let us skip the bag-of-words step by using `vectorizers` to go from text or tokens to a ([sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)) matrix directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term-matrix from the full text\n",
    "tf_full_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_full = tf_full_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Create a document-term-matrix from the selected tokens only\n",
    "tf_segm_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_segm = tf_segm_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))\n",
    "\n",
    "# Create a Term frequency-inverse document frequency (tf-idf) matrix from the selected tokens\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf vs tfidf\n",
    "Let us compare a sample of the term-frequency matrix with the same sample from the term-frequency-inverse-document-frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Term frequency')\n",
    "document_amount = tf_segm.shape[0]\n",
    "token_amount = tf_segm.shape[1]\n",
    "print((tf_segm/token_amount * document_amount)[:,:6].todense())\n",
    "print()\n",
    "\n",
    "print('Term frequency - Inverse document frequency')\n",
    "print(tfidf[:,:6].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the *regular* term-frequency matrix because we want to use LDA in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init LDA to look for 3 topics\n",
    "lda_full = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# a second model for the segmented DTM\n",
    "lda_segm = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# fit the model and transform for results\n",
    "lda_full_results = lda_full.fit_transform(tf_full)\n",
    "lda_segm_results = lda_segm.fit_transform(tf_segm)\n",
    "\n",
    "# read in a new file\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "with open(txt_file, mode='rt', encoding='8859') as byte_file:\n",
    "    new_text = byte_file.read()\n",
    "\n",
    "# transform the new text to a doc-term-matrix\n",
    "tf_full_new = tf_full_vectorizer.transform([new_text])\n",
    "tf_segm_new = tf_segm_vectorizer.transform([new_text])\n",
    "# and transform to discover associated topics\n",
    "lda_full_new = lda_full.fit_transform(tf_full_new)\n",
    "lda_segm_new = lda_segm.fit_transform(tf_segm_new)\n",
    "print(lda_full_new)\n",
    "print(lda_segm_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(lda_full, tf_full_vectorizer.get_feature_names(), 9)\n",
    "print_top_words(lda_segm, tf_segm_vectorizer.get_feature_names(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier komt nog feedback op de gegeven topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other\n",
    "\n",
    "Usefull libs:\n",
    "\n",
    "- re\n",
    "- gensim\n",
    "- spaCy\n",
    "- polyglot\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "Popular NL classifier:\n",
    "- Naive-Bayes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
