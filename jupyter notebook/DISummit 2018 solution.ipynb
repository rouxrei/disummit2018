{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiSummit 2018 - Introduction to text mining\n",
    "<img src=\"XploData_logo.png\" width = \"40%\">\n",
    "<br>\n",
    "Powered by [XploData](https://www.xplodata.be/)\n",
    "<br>\n",
    "Reinert Roux - Jellert Schaepherders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text mining is a general term used to indicate the retrieval of information from (large quantities of) text. What can be done by hand (reading text, knowing what it is about, determining keywords, associating it with other texts...) can to some extent also be done by using machine learning algorithms, be it much faster. In this hands-on Python workshop, we will introduce you to some of the basic text mining principles.\n",
    "\n",
    "In the context of the [HIV Hackathon](https://hivhack.org/), organized by DigitYser in September 2018, we will mine text from multiple PDFs related to HIV. For example, this [publicly available PDF](http://phia.icap.columbia.edu/wp-content/uploads/2017/11/Tanzania_SummarySheet_A4.English.v19.pdf) related to HIV in Tanzania. Although the PDFs contains multiple types of data, the scope of this workshop will be limited to the actual text itself. \n",
    "<br>\n",
    "*Note that text can also be extracted from images using [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition).*\n",
    "\n",
    "Before we can start with text mining, we need to acquire the text from our PDFs and load it into Python.\n",
    "Fortunately, multiple Python libraries exist. We will use the popular PyPDF2 package.\n",
    "For flexibility reasons, we will also introduce the XpdfReader toolkit.\n",
    "\n",
    "## Importing data\n",
    "### Extracting text with a python library\n",
    "e.g.: [PyPDF2](https://pypi.org/project/PyPDF2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project setup\n",
    "import os\n",
    "\n",
    "# change working directory\n",
    "os.chdir('..')\n",
    "project_root = os.getcwd()\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# creating an object \n",
    "file_path = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "file = open(file_path, 'rb')\n",
    "\n",
    "# creating a pdf reader object\n",
    "fileReader = PyPDF2.PdfFileReader(file)\n",
    "\n",
    "# print the number of pages in pdf file\n",
    "print(fileReader.numPages)\n",
    "print()\n",
    "\n",
    "# print text on last page\n",
    "print(fileReader.pages[-1].extractText())\n",
    "\n",
    "# close the file-stream\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "    - no control over character encodings\n",
    "    - no image extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text with an external toolkit\n",
    "e.g.: [PDFTOTEXT (XpdfReader)](http://www.xpdfreader.com/)\n",
    "\n",
    "The commandline can be accessed using the `subprocess` standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'utf8_encoded.txt')\n",
    "\n",
    "# make and execute command to convert to txt using UTF-8 encoding for resulting txt file\n",
    "pdf_txt_command = [\"pdftotext\", \"-enc\", \"UTF-8\", in_file, txt_file]\n",
    "subprocess.run(pdf_txt_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now open the textfile with the specified encoding\n",
    "with open(txt_file, mode='rt', encoding='utf-8') as file:\n",
    "    print(file.read()[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that we used the argument 'UTF-8' for the encoding parameter. Variable width character encoding is used to represent a repertoire of characters by an encoding system (more on that [here](https://en.wikipedia.org/wiki/Character_encoding)). UTF-8 is one of the most common used encodings since 2009, but if you are not sure of the character encoding of the pdf-file, you can choose to not specify it during the conversion and make a *guess* later.\n",
    "We can use the `chardet` package for *guessing* character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# make and execute command to convert to txt, without specifying an encoding\n",
    "pdf_txt_command = [\"pdftotext\", in_file, txt_file]\n",
    "subprocess.run(pdf_txt_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# define input file path\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# read file as bytes\n",
    "with open(txt_file, mode='rb') as byte_file:\n",
    "    raw_input = byte_file.read()\n",
    "\n",
    "# guess encoding\n",
    "encodings = chardet.detect(raw_input)\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode bytes to string\n",
    "text = raw_input.decode('8859')\n",
    "# show text sample\n",
    "print(text[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will continue using the `ISO-8859-1` decoded text.\n",
    "\n",
    "For more information on supported codecs, visit the python [docs](https://docs.python.org/3/library/codecs.html#standard-encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting images from a PDF\n",
    "#### Using  [PDFIMAGES (XpdfReader)](http://www.xpdfreader.com/)\n",
    "\n",
    "PDFimages come from the same toolkit as pdftotext and we also call it using the `subprocess` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define file paths\n",
    "in_file = os.path.join(project_root, 'data','HIV','Tanzania_SummarySheet_English.pdf')\n",
    "images = os.path.join(project_root, 'output', 'output_img')\n",
    "\n",
    "# extract images\n",
    "pdf_img_command = [\"pdfimages\", \"-j\", in_file, os.path.join(images + 'img')]\n",
    "subprocess.run(pdf_img_command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that a lot of the extracted images are in a ppm or similar format, which few programs can easily work with.\n",
    "A useful toolkit for working with images is [imagemagick](https://www.imagemagick.org/script/index.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection\n",
    "A human capable of reading is able to distinguish between his mother tongue and a foreign language. We perceive this by reading language specific words, grammatical constructions, context... Language detection in Python is quite straightforward and is performed using the `langdetect` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "language = detect(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Until this point, you were mainly preparing the data, i.e. converting the pdf to text and analyzing some meta-data.\n",
    "The next critical step in text mining is preprocessing. This process involves different techniques (for an overview, see [here](https://pdfs.semanticscholar.org/1fa1/1c4de09b86a05062127c68a7662e3ba53251.pdf)), of which we will cover **tokenization, part-of-speech tagging, stop word removal, stemming and lemmatization**.\n",
    "### Wordclouds and bar charts\n",
    "To get insights in the contents of a text file, various visualisations are possible. Frequently used visualisations are wordclouds and bar charts. These will therefore be used to demonstrate the different aspects of text mining preprocessing.\n",
    "\n",
    "*Since we will often repeat the same visualisations during this workshop, we prepared a custom function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# instantiate wordcloud\n",
    "wordcloud = WordCloud(  background_color='white',\n",
    "                        max_words=50,\n",
    "                        max_font_size=80, \n",
    "                        random_state=42,\n",
    "                        collocations=False)\n",
    "\n",
    "def gen_wc_barh(tokens, title='Default title', amount=9):\n",
    "    \"\"\"\n",
    "    Generate a horizontal bar graph from the top #amount tokens.\n",
    "    Generate a wordcloud from the provided tokens.\n",
    "    Show both vizualizations.\n",
    "    \"\"\"\n",
    "    # count tokens\n",
    "    ctr = Counter(tokens)\n",
    "    \n",
    "    # initialize plt figure\n",
    "    plt.figure(figsize=(12,9))\n",
    "    \n",
    "    # generate barh from counted tokens\n",
    "    tokens, weights = zip(*ctr.most_common(amount))\n",
    "    plt.subplot2grid((3, 3), (0, 0))\n",
    "    plt.barh(tokens, weights)\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Token count bar-graph')\n",
    "    \n",
    "    # generate wc from the counted tokens\n",
    "    wordcloud.generate_from_frequencies(dict(ctr))\n",
    "    plt.subplot2grid((3, 3), (0, 1), colspan=2)\n",
    "    plt.imshow(wordcloud, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word cloud')\n",
    "\n",
    "    # general title\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "        \n",
    "    # show both graphs\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the importance of preprocessing, we will generate a visualisation after each preprocessing step, whereafter you can evaluate the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)\n",
    "One of the core python packages we will use during this workshop is `nltk`.\n",
    "You can install this package like any other python package (using conda or pip etc...) but this does not install everything. A lot of the functions in this package depend on prebuild corpora and models that need to be downloaded separately. Depending on how you are using this notebook, the downloads migth have already been done. If you get any errors using `nltk` functions, check the error for mentions of missing downloads.\n",
    "\n",
    "*This notebook uses the following nltk packages:*\n",
    " - averaged_perceptron_tagger \n",
    " - maxent_ne_chunker\n",
    " - punkt\n",
    " - wordnet\n",
    " - words\n",
    " - stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "[Tokenization]( https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of cutting the text files in individual *tokens*. These can be words, but also numbers, punctuation marks or symbols. However, don’t you worry about that just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# break text into smaller bits -> tokens\n",
    "# First, split the text in sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "# Then we further split each sentence into tokens\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate all the tokenized sentences into a single list\n",
    "gen_wc_barh([token for sent in tokenized_sentences for token in sent], title='Tokenized text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, you can see that the extracted tokens resulted in more than just 'words'. What do you think was used as delimiter (or separator) for splitting the tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tagging\n",
    "To increase the value of our freshly extracted tokens, we can assign labels/tags to them. As a result, each token is given a grammatical meaning. This process is called [Part-Of-Speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). The ‘tagging’ is based on a previously trained model, which we can also call from the `nltk` package. The default tags generated by `nltk` can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model built into NLTK to assign grammatical meaning to our tokens -> POS tags\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "print(\"Example tagged sentence:\")\n",
    "print(tagged_sentences[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing our text, we usually aren't interested in all types of tokens. For our usecase we are interested in discovering the topics of our text, so we are more interested in 'word-like' tokens. Therefore, let's select the appropriate tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which tokens we want to consider\n",
    "# shorthand POS tag lists:\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# interesting tokens\n",
    "interesting_tokens = [(token, tag)\\\n",
    "            for sentence in tagged_sentences\\\n",
    "            for (token, tag) in sentence\\\n",
    "            if token.isalpha()\\\n",
    "            and tag in adj + noun + adverb + verb]\n",
    "\n",
    "# extract the 'token-values' from the tagged sentences\n",
    "gen_wc_barh([token for (token, tag) in interesting_tokens], title='Interesting tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tagging is not always ‘sure’ about the meaning of a word. Could you determine which (of the) word(s) in the above bar chart has/have multiple tagging possibilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence. These words are called stop words. How many stop words do you think are in this paragraph? To better understand the content of a text, you want to filter out those stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# filter out the English stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "filtered_tokens = [(token, tag) for (token,tag) in interesting_tokens if token not in stopwords]\n",
    "\n",
    "# re-use alias 'tokens' for readability in following code\n",
    "# -> contains only the tokens and dropped the tags\n",
    "tokens = [token for (token, tag) in filtered_tokens]\n",
    "\n",
    "# extract the 'token-values' from the filtered tokens \n",
    "gen_wc_barh(tokens, title='Stopwords removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the changes in the wordcloud with and without the removal of stop words? Which words were omitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is a preprocessing step whereby inflected words are reduced to their word stem, base or root form. To give an example, the words “computational”, “computers” and “computation” would, according to a predefined algorithm, result in the stem word “comput” after the stemming process. Consequently, words with a similar stem will be grouped together and won’t skew the word count/frequency distribution. However, “comput” isn’t a real word and could obscure the interpretation of the text. Nevertheless, the code is not very complicated as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stem tokens using the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "gen_wc_barh(stemmed_tokens, title='Stemmed tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a side note: stemming could be followed by a stem completion algorithm, which completes the stemmed words (e.g. \"comput\") to their meaningful counterparts (e.g. \"computer\"), based on a predefined completion dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "While stemming uses predetermined rules to get to the *stem* version of a word, [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is based on a trained model and is aware of word *meanings*. For example: “are” and “be” will remain two different words after stemming, but will be changed into 'is' after the lemmatization process. Still, a well-trained algorithm can distinguish between similar words with different meanings (e.g. 'viral' and 'virally'), so they are not combined during lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem.wordnet as wordnet\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize interesting tokens\n",
    "lemmatizer = wordnet.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "gen_wc_barh(lemmatized_tokens, title='Lemmatized tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main differences between these visuals and the previous ones? Which one is more useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "So far, what you mainly have been doing, is a so called [bag-of-words analysis](https://en.wikipedia.org/wiki/Bag-of-words_model). This simplified model literally throws all words (or tokens, if you wish) of a document into a ‘bag’ and then looks for the most occurring ones. Now, we will do this for a range of documents.\n",
    "\n",
    "We will start with reading multiple documents and putting them in a [pandas](https://pandas.pydata.org/) DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "txt_dir = os.path.join(project_root, 'output', 'medicine_txt')\n",
    "\n",
    "file_name = []\n",
    "input_text = []\n",
    "\n",
    "# read all txt files (one time)\n",
    "for curr_dir,_,filenames in os.walk(txt_dir):\n",
    "    for filename in filenames:\n",
    "        # filter to select only the pdfs that were converted using the 'simple' option\n",
    "        if filename[:7] == 'simple_':\n",
    "            # decode using the iso-8859 character set\n",
    "            with open(os.path.join(curr_dir, filename), 'rt', encoding='8859') as file:\n",
    "                txt_input = file.read()\n",
    "                # consider only files with at least 100 characters\n",
    "                if len(txt_input) > 99:\n",
    "                    # strip '.txt' from the filename\n",
    "                    file_name.append(filename[7:-4])\n",
    "                    input_text.append(txt_input)\n",
    "\n",
    "# store the documents in a pd.DataFrame\n",
    "df = pd.DataFrame({'filename':file_name, 'text':input_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a small DataFrame containing the filename and text-content of our documents. We can do some of the preprocessing introduced earlier and store the results in extra columns.\n",
    "\n",
    "*This step can take a few minutes ...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "df['tokens'] = df['text'].apply(lambda txt: [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(txt)])\n",
    "# POS-tagging\n",
    "df['POS_tags'] = df['tokens'].apply(lambda tokens: [nltk.pos_tag(sent) for sent in tokens])\n",
    "\n",
    "# split on tags\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#adverb = ['RB', 'RBR', 'RBS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# select only tokens that were tagged as 'adjective', 'noun' or 'verb'.\n",
    "df['i_tokens'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if token.isalpha() and tag in adj + noun + verb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first feeling of the resulting tokens we can use two collection classes built-in to standard python: `set` and `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# get a set of unique tokens in our documents\n",
    "df['set_of_words'] = df['i_tokens'].apply(set)\n",
    "# print the results\n",
    "for index,value in df['set_of_words'].items():\n",
    "    print('Document {0} contains {1} unique words.'.format(index, len(value)))\n",
    "print()    \n",
    "\n",
    "# count the tokens using a 'Counter' collection\n",
    "df['count_of_words'] = df['i_tokens'].apply(Counter)\n",
    "# print the top 9 words of each document\n",
    "for index,value in df['count_of_words'].items():\n",
    "    print('Document {0} top words are {1}.'.format(index, value.most_common(9)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Term Matrix\n",
    "From the `i_token` column of the DataFrame, one can construct a [document-term-matrix **(DTM)**](https://en.wikipedia.org/wiki/Document-term_matrix).\n",
    "Simply put, a DTM is just another mathematical representation of multiple bag of word analyses, with rows corresponding to the documents in the corpus and columns corresponding to the terms (tokens/words). The values are the term (word/token) frequencies per document. One can also normalize these frequencies by dividing by the total number of words per document.\n",
    "\n",
    "With `sklearn`, you can use `vectorizers` to create a DTM to go from text or tokens to a ([sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)) matrix representing such a DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Create a document-term-matrix from the full text\n",
    "tf_full_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_full = tf_full_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Create a document-term-matrix from the selected tokens only\n",
    "tf_segm_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_segm = tf_segm_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at a small sample of the tf_segm matrix.\n",
    "\n",
    "Let's also normalize this matrix with the total amount of words in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DTM')\n",
    "print(tf_segm[:,:4].todense())\n",
    "print()\n",
    "\n",
    "print('Term frequency')\n",
    "# compute the term-frequency by dividing by total words in the document\n",
    "term_freq = tf_segm / tf_segm.sum(axis=1)\n",
    "print(term_freq[:,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF) vs Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "Instead of considering the raw term frequencies per document, one could also account for the frequencies in the corpus. The sparseness of a term is measured by its inverse document frequency. The product of the TF and IDF gives you the TF-IDF. Whichever you use, depends on the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Term frequency-inverse document frequency (tf-idf) matrix from the selected tokens\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))\n",
    "\n",
    "print('Term frequency - Inverse document frequency')\n",
    "print(tfidf[:,:4].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare a sample of the term-frequency matrix with the same sample of the term frequency-inverse document frequency matrix. \n",
    "\n",
    "*We will look at a couple of words from the 7th document.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "tf_df = pd.DataFrame(term_freq, columns=tf_segm_vectorizer.get_feature_names())\n",
    "fig, ax = plt.subplots(1)\n",
    "tf_df.iloc[7,8:19].T.plot(kind='bar')\n",
    "plt.title('Term Frequency')\n",
    "rect = Rectangle((-.4,0.00029), 1.8,0.00011, fill = False, linewidth = 2, color = 'r')\n",
    "rect2 = Rectangle((8-.4,0.000055), 1.8,0.00005, fill = False, linewidth = 2, color = 'r')\n",
    "ax.add_patch(rect)\n",
    "ax.add_patch(rect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names())\n",
    "fig, ax = plt.subplots(1)\n",
    "tfidf_df.iloc[7,8:19].T.plot(kind='bar')\n",
    "plt.title('Term Frequency - Inverse Document Frequency')\n",
    "rect = Rectangle((-.4,0.006), 1.8,0.0053, fill = False, linewidth = 2, color = 'r')\n",
    "rect2 = Rectangle((8-.4,0.0025), 1.8,0.0015, fill = False, linewidth = 2, color = 'r')\n",
    "ax.add_patch(rect)\n",
    "ax.add_patch(rect2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these two bar charts, you can tell that some of the words had their *weights* shifted. For example, comparing the word **able** with **abnormal**, we can see that **able** loses some significance compared to **abnormal** in the TF-IDF matrix.\n",
    "This indicates that it's a word that occurs in more documents thus makes it less likely that it's an identifying or unique word in the current document and thus has **less power to differentiate between documents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "After all preprocessing steps, the *real* fun begins. Where POS tagging was limited to tag individual words only, [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is able to group multiple tokens into predefined categories, such as person names, locations, organizations, products, time... \n",
    "We'll give an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "# take a 'random' sentence\n",
    "sentence = list(chunked_sentences)[-4]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, you can recognize the found 'Named Entities' by the prefix `(NE`. We can now use the same wordcloud and bar chart visualizations to highlight the most frequent occurring named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "# filter out everything else\n",
    "named_entities = [chunk.leaves() for sent in chunked_sentences for chunk in sent if hasattr(chunk, \"label\") and chunk.label() == \"NE\"]\n",
    "NE_tokens = [token for leaves in named_entities for (token, tag) in leaves]\n",
    "\n",
    "gen_wc_barh(NE_tokens, title='Named Entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic detection & prediction\n",
    "Instead of doing a simple bag-of-words analysis, let's do something more fancy: topic detection! \n",
    "\n",
    "To this end, we will be using the popular [latent Dirichlet allocation model **(LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "This model can also be used to predict the topics of new, unseen documents.\n",
    "\n",
    "An LDA model is essentially used to discover topics in documents based on a variety of variables, such as the number of topics, number of documents, probability and distribution of words, identity and weights of the words etc. The math behind the model is mind-boggling (have a look [here]( https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution)). Luckily for us, calling the LDA model in Python is not that hard.\n",
    "\n",
    "*Note that for the purpose of this workshop we will keep the model simple. As an exercise you can apply more of the preprocessing techniques we discussed earlier  (notably stemming or lemmatizing) and experiment with the parameters of the model itself.*\n",
    "\n",
    "We use the DTM we computed earlier to train the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# init LDA to look for 3 topics, using the DTM \n",
    "lda_segm = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# fit the model and transform for results\n",
    "lda_segm_results = lda_segm.fit_transform(tf_segm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now come up with `n=3` topics and calculated which features determine that a document corresponds to a certain topic.\n",
    "\n",
    "Let's have a look at the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    \n",
    "print_top_words(lda_segm, tf_segm_vectorizer.get_feature_names(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we printed out here are the top features for each topic, it does not have any meaning beyond that. You can however label these topics yourself or ask a business user to do so based on the top features or some example documents.\n",
    "\n",
    "We can now use this model on a new document to classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a new file\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "with open(txt_file, mode='rt', encoding='8859') as byte_file:\n",
    "    new_text = byte_file.read()\n",
    "\n",
    "# transform the new text to a doc-term-matrix\n",
    "tf_segm_new = tf_segm_vectorizer.transform([new_text])\n",
    "\n",
    "# and transform to discover associated topics\n",
    "lda_segm_new = lda_segm.fit_transform(tf_segm_new)\n",
    "print(lda_segm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we get here can be seen as the probability that this documents belongs to the various topics. From the values we can conclude that the document belongs to `Topic#1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other\n",
    "\n",
    "Usefull libs:\n",
    "\n",
    "- re\n",
    "- gensim\n",
    "- spaCy\n",
    "- polyglot\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "Popular NL classifier:\n",
    "- Naive-Bayes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
