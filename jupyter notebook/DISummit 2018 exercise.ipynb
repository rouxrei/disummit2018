{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiSummit 2018 - Introduction to text mining \n",
    "## Exercise version\n",
    "<img src=\"XploData_logo.png\" width = \"40%\">\n",
    "<br>\n",
    "Powered by [XploData](https://www.xplodata.be/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text mining is a general term to retrieve information from (large quantities of) text. What can be done by hand (reading a text, knowing what it is about, determining keywords, associating with other texts...) can to some extent also be done my using machine learning algorithms, be it much faster. In this hands-on Python workshop, we will introduce you to some of the text mining principles that are used to achieve this.\n",
    "\n",
    "In the context of the [HIV Hackathon](https://hivhack.org/), organized by Digityser in September 2018, we will focus on a [public available PDF](http://phia.icap.columbia.edu/wp-content/uploads/2017/11/Tanzania_SummarySheet_A4.English.v19.pdf) related to HIV in Tanzania. Although the PDF contains multiple types of data, the scope of this workshop will be limited to the actual text itself. \n",
    "*As a side note, alternatively text can be extracted from images using [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition).*\n",
    "\n",
    "Before we can start with text mining, we need to acquire the text from our PDF into Python.\n",
    "Fortunately, multiple Python libraries exist, but we will use the popular PyPDF2 package.\n",
    "For flexibility reasons, we will also introduce the XpdfReader toolkit.\n",
    "\n",
    "### Exercises\n",
    "This notebook is the exercise version of the workshop. Feel free to check the solution version.\n",
    "In this version some code parts are left out and you need to fill it in yourself.\n",
    "Places where your input is expected will look like this:\n",
    "\n",
    "`var = ##your code##`\n",
    "\n",
    "Make sure to replace all the `#` so that the code can be executed.\n",
    "\n",
    "\n",
    "## Importing data\n",
    "\n",
    "For the exercises we assume that we already have a txt version for al our pdf files.\n",
    "For more info on how to achieve those txt files, look at the solution version of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project setup\n",
    "import os\n",
    "\n",
    "# change working directory\n",
    "os.chdir('..')\n",
    "project_root = os.getcwd()\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# define input file path\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "\n",
    "# read file as bytes\n",
    "with open(txt_file, mode='rb') as byte_file:\n",
    "    raw_input = byte_file.read()\n",
    "\n",
    "# guess encoding\n",
    "encodings = chardet.detect(raw_input)\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode bytes to string\n",
    "text = raw_input.decode('8859')\n",
    "# show text sample\n",
    "print(text[-900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue the rest of the examples with the `ISO-8859-1` decoded text.\n",
    "\n",
    "For more info on supported codecs, visit the python [docs](https://docs.python.org/3/library/codecs.html#standard-encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection\n",
    "A human capable of reading is able to distinguish between his mother tongue and a foreign language. We perceive this by reading language specific words, grammatical constructions, context... Language detection in Python is quite straightforward and is performed using the `langdetect` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "language = detect(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Until this point, you were mainly preparing the data, i.e. converting the pdf to text and analyzing some meta-data.\n",
    "The next critical step in text mining is preprocessing. This process involves different techniques (for an overview, see [here](https://pdfs.semanticscholar.org/1fa1/1c4de09b86a05062127c68a7662e3ba53251.pdf)), of which we will cover **tokenization, part-of-speech tagging, stop word removal, stemming and lemmatization**.\n",
    "### Wordclouds and bar charts\n",
    "To get insights of the contents of a text file, various visualisations are possible. Frequently used visualisations are wordclouds and bar charts, and will be used to demonstrate the differenct aspects of text mining preprocessing.\n",
    "\n",
    "*Since we will often repeat the same visualisation during this workshop, we prepared a custom function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# instantiate wordcloud\n",
    "wordcloud = WordCloud(  background_color='white',\n",
    "                        max_words=50,\n",
    "                        max_font_size=80, \n",
    "                        random_state=42,\n",
    "                        collocations=False)\n",
    "\n",
    "def gen_wc_barh(tokens, title='Default title', amount=9):\n",
    "    \"\"\"\n",
    "    Generate a horizontal bar graph from the top #amount tokens.\n",
    "    Generate a wordcloud from the provided tokens.\n",
    "    Show both vizualizations.\n",
    "    \"\"\"\n",
    "    # count tokens\n",
    "    ctr = Counter(tokens)\n",
    "    \n",
    "    # initialize plt figure\n",
    "    plt.figure(figsize=(12,9))\n",
    "    \n",
    "    # generate barh from counted tokens\n",
    "    tokens, weights = zip(*ctr.most_common(amount))\n",
    "    plt.subplot2grid((3, 3), (0, 0))\n",
    "    plt.barh(tokens, weights)\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Token count bar-graph')\n",
    "    \n",
    "    # generate wc from the counted tokens\n",
    "    wordcloud.generate_from_frequencies(dict(ctr))\n",
    "    plt.subplot2grid((3, 3), (0, 1), colspan=2)\n",
    "    plt.imshow(wordcloud, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word cloud')\n",
    "\n",
    "    # general title\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "        \n",
    "    # show both graphs\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the importance of preprocessing, we will generate a visualisation after each preprocessing step, whereafter you can evaluate the effect. For starters, we build our first visual on the *unpreprocessed* text, to get a feeling of what a wordcloud can tell you about the contents of a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from unprocessed input text\n",
    "wordcloud.generate_from_text(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how frequent occurring words are displayed in a larger font size, but these are not necessarily the most important ones (e.g. percent, year, among). You can improve this, let’s do this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "[Tokenization]( https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of cutting the text files in individual ‘tokens’. These can be words, but also numbers, punctuation marks or symbols, but don’t worry for that now. You’ll need an additional package for this, called the \"Natural Language Toolkit\": `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Use `nltk.sent_tokenize` to split the text into sentences.\n",
    "\n",
    "Use `nltk.word_tokenize` to split a sentence into tokens.\n",
    "\n",
    "*Note that both functions take a `str` as input and return a list, so you will need to use a list-comprehension to apply `nltk.word_tokenize` on an individual sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# get the input text\n",
    "text = raw_input.decode('8859')\n",
    "\n",
    "# break text up into smaller bits -> tokens\n",
    "# First, split the text up in sentences\n",
    "sentences = ##your code##\n",
    "# Then we further split each sentence into tokens\n",
    "tokenized_sentences = [##your code##]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate all the tokenized sentences into a single list\n",
    "gen_wc_barh([token for sent in tokenized_sentences for token in sent], title='Tokenized text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, you can see that the extracted tokens resulted in more than just 'words'. What do you think was used as delimiter (or separator) for splitting up the tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tagging\n",
    "To increase the value of our freshly extracted tokens, we can assign labels/tags to them. As a result, each token is given a grammatical meaning. This process is called [Part-Of-Speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). The ‘tagging’ is based on a previously trained model, which we can also call from the `nltk` package. The default tags generated by `nltk` can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "#### Exercise\n",
    "Use `nltk.pos_tag` to perform POS tagging on the tokens.\n",
    "\n",
    "This functions expects a list of tokens and returns a list of tuples `(token, tag)`\n",
    "\n",
    "*So again, you will have to use a list-comprehension to apply `nltk.pos_tag` on a sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model built into NLTK to assign grammatical meaning to our tokens -> POS tags\n",
    "tagged_sentences = [##your code##]\n",
    "print(\"Example tagged sentence:\")\n",
    "print(tagged_sentences[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing our text, we usually aren't interested in all types of tokens. For our usecase we are interested in discovering the topics of our text, so we are more interested in 'word-like' tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which tokens we want to consider\n",
    "# shorthand POS tag lists:\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# interesting tokens\n",
    "tokens = [(token, tag)\\\n",
    "            for sentence in tagged_sentences\\\n",
    "            for (token, tag) in sentence\\\n",
    "            if token.isalpha()\\\n",
    "            and tag in adj + noun + adverb + verb]\n",
    "\n",
    "# extract the 'token-values' from the tagged sentences\n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Filtered tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tagging is not always ‘sure’ about the meaning of a word. Could you determine which (of the) word(s) in the above bar chart has/have multiple tagging possibilities?\n",
    "\n",
    "#### Exercise\n",
    "Different Textmining applications require different preprocessing.\n",
    "For exmple in sentiment analysis we're mostly interested in adjectives.\n",
    "Or maybe we're analysing certain verb usage per category, ...\n",
    "\n",
    "**Apply different filters on the code above to get a different set of tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence (how many stop words are in this sentence?). To better understand the content of a text, you want to filter out those stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# filter out the english stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokens = [(token, tag) for (token,tag) in tokens if token not in stopwords]\n",
    "\n",
    "# extract the 'token-values' from the filtered tokens \n",
    "gen_wc_barh([token for (token, tag) in tokens], title='Stopwords removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the changes in the wordcloud with and without the removal of stop words? Which words were omitted?\n",
    "\n",
    "#### Exercise\n",
    "Check if you guessed correctly which words in the paragraph above are stopwords by filtering them out using the `stopwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Spoken languages contain a lot of (small) words that usually don't add extra meaning to a sentence (how many stop words are in this sentence?). To better understand the content of a text, you want to filter out those stop words.\"\n",
    "\n",
    "# tokenize the paragraph\n",
    "paragraph_tokenized = ##your code##\n",
    "\n",
    "# remove the stopwords\n",
    "paragraph_no_stops = ##your code##\n",
    "\n",
    "# print the result\n",
    "print(' '.join([token for (token, tag) in paragraph_no_stops]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is a preprocessing step whereby inflected words are reduced to their word stem, base or root form. To give an example, the words “computational”, “computers” and “computation” would, according to a predefined algorithm, result in the stem word “comput” after the stemming process. Consequently, words with a similar stem will be grouped together and won’t skew the word count/frequency distribution. However, “comput” isn’t a real word and could obscure the interpretation of the text. Nevertheless, the code is uncomplicated as shown below:\n",
    "\n",
    "#### Exercise\n",
    "Use the method `stem` of the initialized stemmer to stem all the `tokens`.\n",
    "\n",
    "It takes as input a `str` and returns a `str`.\n",
    "\n",
    "*Note that `tokens` is a list of tuples `(token, tag)`, so again list-comprehensions are our friend and make sure you apply the method to the token only, not the whole tuple.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stem tokens using the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [##your code##]\n",
    "\n",
    "gen_wc_barh(stemmed_tokens, title='Stemmed tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a side note: stemming could be followed by a stem completion algorithm, which completes the stemmed words (e.g. \"comput\") to their meaningful counterparts (e.g. \"computer\"), based on a predefined completion dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "While stemming uses predetermined rules to get to the *stem* version of a word, [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is based on a trained model and is aware of word *meanings*. For example: “are” and “be” will remain two different words after stemming, but will be changed into 'is' after the lemmatization proces. Still, a well-trained algorithm can distinguish between similar words with different meanings (e.g. 'viral' and 'virally'), so they are not combined during lemmatization.\n",
    "For our lemmatization we are using the 'wordnet' model. When using this for the first time, you need to **download this model first**.\n",
    "\n",
    "#### Exercise\n",
    "Use the method `lemmatize` of the initialized lemmatizer to lemmatize all the `tokens`.\n",
    "\n",
    "It takes as input a `str` and returns a `str`.\n",
    "\n",
    "*Note that `tokens` is a list of tuples `(token, tag)`, so again list-comprehensions are our friend and make sure you apply the method to the token only, not the whole tuple.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem.wordnet as wordnet\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize interesting tokens\n",
    "lemmatizer = wordnet.WordNetLemmatizer()\n",
    "lemmatized_tokens = [##your code##]\n",
    "gen_wc_barh(lemmatized_tokens, title='Lemmatized tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main differences between these visuals and the previous ones? Which one is more useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-o-w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "After al preprocessing steps, the fun *really* begins. Where POS tagging was limited to tag individual words only, [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is able to group multiple tokens into predefined categories, such as person names, locations, organizations, products, time... \n",
    "We'll give an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "# take a 'random' sentence\n",
    "sentence = list(chunked_sentences)[-4]\n",
    "print(sentence)\n",
    "# You can get a more graphical drawing (will show up in a pop-up window)\n",
    "#sentence.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, you can recognize the found 'Named Entities' by the prefix `(NE`. We can now use the same wordcloud and bar chart visualizations to highlight the most frequent occurring named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover named entities based on POS tags\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "# filter out everything else\n",
    "named_entities = [chunk.leaves() for sent in chunked_sentences for chunk in sent if hasattr(chunk, \"label\") and chunk.label() == \"NE\"]\n",
    "NE_tokens = [token for leaves in named_entities for (token, tag) in leaves]\n",
    "\n",
    "gen_wc_barh(NE_tokens, title='Named Entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "During visualisation in the previous steps we already implicitly used bag-of-words to pass onto the wordcloud and bar graph generators. Basically it boils down to counting the tokens we are interested in.\n",
    "Python has some useful built-in classes for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Set, Counter\n",
    "\n",
    "# get a set of unique tokens in our documents\n",
    "set_of_words = set([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The documents contain {} unique words'.format(len(set_of_words)))\n",
    "\n",
    "# count the tokens using a 'Counter' collection\n",
    "count_of_words = Counter([token for token_list in df['i_tokens'] for token in token_list])\n",
    "print('The most common words are:')\n",
    "print(count_of_words.most_common(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency matrix\n",
    "We can combine the bags-of-words into DTMs, however `sklearn` let us skip the bag-of-words step by using `vectorizers` to go from text or tokens to a ([sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)) matrix directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf vs tfidf\n",
    "Let us compare a sample of the term-frequency matrix with the same sample from the term-frequency-inverse-document-frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Term frequency')\n",
    "document_amount = tf_segm.shape[0]\n",
    "token_amount = tf_segm.shape[1]\n",
    "print((tf_segm/token_amount * document_amount)[:,:6].todense())\n",
    "print()\n",
    "\n",
    "print('Term frequency - Inverse document frequency')\n",
    "print(tfidf[:,:6].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the *regular* term-frequency matrix because we want to use LDA in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classification example\n",
    "### Using topic detection\n",
    "So far, what you mainly have been doing, is a so called [bag-of-words analysis](https://en.wikipedia.org/wiki/Bag-of-words_model). This simplified model literally throws all words (or tokens, if you wish) of a document into a ‘bag’ and then looks for the most occurring ones. Now, we will do this for a range of multiple documents, categorizing topics for each document. As a result, you should be able to predict the topics of new, unseen documents.\n",
    "For this, you will need to know about two more text mining techniques: *(i)* convert the bag of words to a [document-term-matrix** (DTM)**](https://en.wikipedia.org/wiki/Document-term_matrix) and *(ii)* the [latent Dirichlet allocation model **(LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "\n",
    "Simply put, A DTM is just another mathematical representation of multiple bag of word analyses, with rows corresponding to the documents in the collection and columns corresponding to the tokens. In Python, you can use the `CountVectorizer` package to create a DTM.\n",
    "An LDA model is essentially used to discover topics in documents, based on a variety of variables, such as the number of topics, number of documents, probability and distribution of words, identity and weights of the words… The math behind the model is mind-boggling (have a look [here]( https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution). Luckily for us, calling the LDA model in Python is not that hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c0dd8813289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#txt_dir = r'..\\output\\medicine_txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtxt_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medicine_txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project_root' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#txt_dir = r'..\\output\\medicine_txt'\n",
    "txt_dir = os.path.join(project_root, 'output', 'medicine_txt')\n",
    "\n",
    "file_name = []\n",
    "input_text = []\n",
    "\n",
    "# read all txt files (one time)\n",
    "for curr_dir,_,filenames in os.walk(txt_dir):\n",
    "    for filename in filenames:\n",
    "        # filter to select only the pdfs that were converted using the 'simple' option\n",
    "        if filename[:7] == 'simple_':\n",
    "            # decode using the iso-8859 character set\n",
    "            with open(os.path.join(curr_dir, filename), 'rt', encoding='8859') as file:\n",
    "                txt_input = file.read()\n",
    "                # consider only files with at least 100 characters\n",
    "                if len(txt_input) > 99:\n",
    "                    # strip '.txt' from the filename\n",
    "                    file_name.append(filename[7:-4])\n",
    "                    input_text.append(txt_input)\n",
    "\n",
    "# store as pd.DataFrame\n",
    "df = pd.DataFrame({'filename':file_name, 'text':input_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "df['tokens'] = df['text'].apply(lambda txt: [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(txt)])\n",
    "\n",
    "# POS-tagging\n",
    "df['POS_tags'] = df['tokens'].apply(lambda tokens: [nltk.pos_tag(sent) for sent in tokens])\n",
    "\n",
    "# split on tags\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "#df['adj'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adj])\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#df['noun'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in noun])\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "#df['adverb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in adverb])\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "#df['verb'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if tag in verb])\n",
    "\n",
    "# select only tokens that were tagged as 'adjective', 'noun' or 'verb'.\n",
    "df['i_tokens'] = df['POS_tags'].apply(lambda tokens: [token for sent in tokens for (token, tag) in sent if token.isalpha() and tag in adj + noun + verb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term-matrix from the full text\n",
    "tf_full_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_full = tf_full_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Create a document-term-matrix from the selected tokens only\n",
    "tf_segm_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf_segm = tf_segm_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))\n",
    "\n",
    "# Create a Term frequency-inverse document frequency (tf-idf) matrix from the selected tokens\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "#tfidf = tfidf_vectorizer.fit_transform(df['i_tokens'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init LDA to look for 3 topics\n",
    "lda_full = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# a second model for the segmented DTM\n",
    "lda_segm = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# fit the model and transform for results\n",
    "lda_full_results = lda_full.fit_transform(tf_full)\n",
    "lda_segm_results = lda_segm.fit_transform(tf_segm)\n",
    "\n",
    "# read in a new file\n",
    "txt_file = os.path.join(project_root, 'output', 'output_txt', 'guess_encoding.txt')\n",
    "with open(txt_file, mode='rt', encoding='8859') as byte_file:\n",
    "    new_text = byte_file.read()\n",
    "\n",
    "# transform the new text to a doc-term-matrix\n",
    "tf_full_new = tf_full_vectorizer.transform([new_text])\n",
    "tf_segm_new = tf_segm_vectorizer.transform([new_text])\n",
    "# and transform to discover associated topics\n",
    "lda_full_new = lda_full.fit_transform(tf_full_new)\n",
    "lda_segm_new = lda_segm.fit_transform(tf_segm_new)\n",
    "print(lda_full_new)\n",
    "print(lda_segm_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(lda_full, tf_full_vectorizer.get_feature_names(), 9)\n",
    "print_top_words(lda_segm, tf_segm_vectorizer.get_feature_names(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier komt nog feedback op de gegeven topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other\n",
    "\n",
    "Usefull libs:\n",
    "\n",
    "- re\n",
    "- gensim\n",
    "- spaCy\n",
    "- polyglot\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "Popular NL classifier:\n",
    "- Naive-Bayes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
